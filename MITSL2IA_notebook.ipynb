{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MITSL2IA evaluation experiments\n",
    "\n",
    "This notebook and general evaluation pipeline was adapted from code written for and used in Chapter 3 of Aksënova (2020). All code in the first few sections was taken verbatim from https://github.com/alenaks/subregular-experiments. The section 'Experiment 10: Multi-tier Input-Sensitive Harmony' was taken from De Santo & Aksënova (2021), with code taken verbatim from https://github.com/alenaks/2IMTSL. The MITSL Experiments section was adapted from Aksënova (2020), by changing to use a MITSL implemented in local_sigmapie/code/mitsl_class.py, which is the main contribution for this work and was adapted from the rest of the code. The python classes in local_sigmapie are needed to run this notebook, as are the natural language corpora files in the natural_data folder.\n",
    "\n",
    "# NEW! String Extension Learning Experiments\n",
    "\n",
    "I adapted the languages contained in this notebook to be a test of the itsl string extension learning adaptations. Look for MITSL Experiment 1 for documentation in comments. Look below the MITSL experiments for further experiments. This notebook tests whether the adapted string extension learner can learn a variety of languages for k=2 (tier dependency window size) and m=2 (symbol width). See Lambert.py for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators and evaluators: the setup for the experiments\n",
    "\n",
    "## Step 1: loading dependencies, including _SigmaPie_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from random import choice, randint\n",
    "from pprint import pprint\n",
    "\n",
    "'''%cd local_sigmapie/code/\n",
    "from main import *\n",
    "%cd ../..'''\n",
    "from Lambert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrong(g, acc, sigstar):\n",
    "    '''\n",
    "    This tests a grammar against an acceptor on a list of strings, and returns every string where the grammar gives the 'wrong' answer\n",
    "    '''\n",
    "    return [w for w in sigstar if scan(g, w) != acc(w)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: defining general harmonic evaluator\n",
    "\n",
    "Here some info on the artificial harmonic generator used here, originally from Chapters 3 and 4 of Aksënova (2020).\n",
    "It can generate two types of samples:\n",
    "\n",
    "* Samples of **well-formed words**, i.e. words that don't violate the rules of the harmony; and\n",
    "* Samples of **underlying -> surface forms**, i.e. pairs where the first member has only the first value of every harmonic class specified (i.e. the feature that needs to be spread is given), and all consecutive members of the same class are masked as the name of that class.\n",
    "\n",
    "### Parameters of the generator\n",
    "\n",
    "List of the parameters that are available:\n",
    "\n",
    "* number of strings to be generated;\n",
    "* harmonic classes and their members (harmonic class is a class of segments that don't co-occur unless there is a blocker in-between them);\n",
    "* minimal and maximal cluster length of each of the harmonic classes;\n",
    "* blockers and the new domain that they introduce;\n",
    "* a probability of observing a blocker (1 / n, where n is a parameter): basically means \"every n-th cluster will be the blocker\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Harmony(object):\n",
    "    \"\"\"\n",
    "    Class defining the toy generator for the harmonic datasets.\n",
    "    \n",
    "    Attributes:\n",
    "        cl_members (dict): dictionary of the type {(harmonic_class_1):class_id_1,\n",
    "            (harmonic_class_2):class_id_2, ...} that contains info about the present\n",
    "            harmonic classes. Note that the transparent element can be encoded by \n",
    "            a harmonic class containing a single element.\n",
    "            Example: {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\", (\"c\"):\"C\"}\n",
    "        cl_lengths (dict): dictionary of the type {class_id:(min_len, max_len)},\n",
    "            where min_len and max_len denote the min and max len of the cluster\n",
    "            made out of elements of class_id.\n",
    "            Example: {\"A\":(1, 3), \"B\":(2, 4), \"C\":(4, 8)}\n",
    "        blockers (dict): dictionary of the type {\"b_1\":\"u_1\", \"b_2\":\"u_2\", ...} where\n",
    "            \"b\" is the blocker, and \"u\" is the newly introduced value.\n",
    "            Example: {\"t\":\"p\"}\n",
    "        blocker_prob (int): a chance of observing a blocker, the P evaluates from\n",
    "            (1/blocker_prob).\n",
    "            Example: 5\n",
    "    \"\"\"\n",
    "    def __init__(self, cl_members, cl_lengths = None, blockers = None, blocker_prob = 5):\n",
    "        \"\"\"\n",
    "        Init function for the Harmony class.\n",
    "        \"\"\"\n",
    "        self.cl_members = cl_members\n",
    "        if cl_lengths is not None:\n",
    "            self.cl_lengths = cl_lengths\n",
    "        else:\n",
    "            self.cl_lengths = {i:(1, 3) for i in self.cl_members.values()}\n",
    "        self.blockers = blockers\n",
    "        self.blocker_prob = blocker_prob\n",
    "        \n",
    "\n",
    "        \n",
    "    def generate_words(self, n = 3, length = 10):\n",
    "        \"\"\"\n",
    "        Generates n strings of a given length.\n",
    "        \n",
    "        Arguments:\n",
    "            n (int): how many strings need to be generated;\n",
    "            length (int): length of the strings.\n",
    "            \n",
    "        Returns:\n",
    "            list[str]: n generated strings.\n",
    "        \"\"\"\n",
    "        # check if the harmony rules are well-formed\n",
    "        if not self._verify_classes():\n",
    "            raise(\"Cannot generate dataset: the sets are overlapping.\")\n",
    "            \n",
    "        # unpack the dictionary for a quicker lookup\n",
    "        unpacked = self._unpack_classes()\n",
    "        transparent = self._transparent()\n",
    "        generated = [self._generate(unpacked, length) for i in range(n)]\n",
    "        return generated\n",
    "    \n",
    "\n",
    "    def generate_pairs(self, n = 3, length = 10):\n",
    "        \"\"\"\n",
    "        Generates n pairs of strings of a given length.\n",
    "        \n",
    "        Arguments:\n",
    "            n (int): how many strings need to be generated;\n",
    "            length (int): length of the strings.\n",
    "            \n",
    "        Returns:\n",
    "            list[tuple[str]]: n generated pairs of strings.\n",
    "        \"\"\"\n",
    "        transparent = self._transparent()\n",
    "        outputs = self.generate_words(n, length)\n",
    "        inputs = self._mask_words(outputs, transparent)\n",
    "        return list(zip(inputs, outputs))\n",
    "        \n",
    "        \n",
    "    def _generate(self, unpacked, length):\n",
    "        \"\"\"\n",
    "        Generates a set of strings; helper function.\n",
    "        \n",
    "        Output type: list[str]\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize the specifications of this particular string\n",
    "        string = \"\"\n",
    "        specs = self._specify()\n",
    "        \n",
    "        while len(string) < length:\n",
    "            \n",
    "            \n",
    "            # check if we can now output the blocker\n",
    "            if self.blockers is not None:\n",
    "                while randint(1, self.blocker_prob) == 1:\n",
    "                    b = choice(list(self.blockers))\n",
    "                    string += b\n",
    "                    \n",
    "                    if len(string) == length:\n",
    "                        return string\n",
    "                    \n",
    "                    # rewrite the specification because of the blocker\n",
    "                    if self.blockers[b] not in specs:\n",
    "                        for spec in specs:\n",
    "                            if unpacked[spec] == unpacked[self.blockers[b]]:\n",
    "                                specs.remove(spec)\n",
    "                                specs.append(self.blockers[b])\n",
    "                                break\n",
    "                                \n",
    "            # make sure that we don't generate cluster of the same\n",
    "            # harminic set as the previous one\n",
    "            if len(string) > 0:\n",
    "                change = string[-1] in unpacked\n",
    "            else:\n",
    "                change = False\n",
    "            \n",
    "            # select and add new possible character as many times as\n",
    "            # cl_lengths indicate\n",
    "            if not change:\n",
    "                newchar = choice(specs)\n",
    "            else:\n",
    "                collection = [i for i in specs]\n",
    "                collection.remove(string[-1])\n",
    "                newchar = choice(collection)\n",
    "            freq_b, freq_e = self.cl_lengths[unpacked[newchar]]\n",
    "            string += newchar * randint(freq_b, freq_e)\n",
    "            \n",
    "            # output\n",
    "            if len(string) > length:\n",
    "                string = \"\"\n",
    "            elif len(string) == length:\n",
    "                return string\n",
    "            \n",
    "            \n",
    "    def _mask(self, string, transparent):\n",
    "        \"\"\"\n",
    "        Masks all non-initial mentions of the specified allophone: helper function.\n",
    "        \n",
    "        Output type: str\n",
    "        \"\"\"\n",
    "        classes = {i:False for i in self.cl_members.keys()}\n",
    "        undergoers = self._undergoers()\n",
    "        new = \"\"\n",
    "        for s in string:\n",
    "            if (s in undergoers) and (s not in transparent.values()):\n",
    "                for c in classes:\n",
    "                    \n",
    "                    # rewrite the non-initial mention of the harmonic set member\n",
    "                    # as its harmony_class_id\n",
    "                    if s in c and not classes[c]:\n",
    "                        classes[c] = True\n",
    "                        new += s\n",
    "                    elif s in c:\n",
    "                        new += self.cl_members[c]\n",
    "            else:\n",
    "                new += s\n",
    "        return new\n",
    "\n",
    "    \n",
    "    def _mask_words(self, words, transparent):\n",
    "        \"\"\"\n",
    "        Masks every word of a given list; helper function.\n",
    "        \n",
    "        Output type: list[str]\n",
    "        \"\"\"\n",
    "        return [self._mask(w, transparent) for w in words]\n",
    "            \n",
    "            \n",
    "    def _undergoers(self):\n",
    "        \"\"\"\n",
    "        Collects all undergoers; helper function.\n",
    "        \n",
    "        Output type: list[char]\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        for i in self.cl_members:\n",
    "            items.extend(list(i))\n",
    "        return items\n",
    "    \n",
    "    def _transparent(self):\n",
    "        \"\"\"\n",
    "        Checks if there are transparent items, i.e. if there is\n",
    "        a harmonic class or classes that only contain a single item.\n",
    "        \n",
    "        Output type: dict[str:str]\n",
    "        \"\"\"\n",
    "        transparent = dict()\n",
    "        for i in self.cl_members:\n",
    "            if len(i) == 1:\n",
    "                transparent[self.cl_members[i]] = i[0]\n",
    "        return transparent\n",
    "        \n",
    "        \n",
    "    def _verify_classes(self):\n",
    "        \"\"\"\n",
    "        Verifies that no set (harmonic sets or the set of blockers)\n",
    "        overlaps with each other.\n",
    "        \n",
    "        Output type: bool\n",
    "        \"\"\"\n",
    "        items = self._undergoers()\n",
    "        if self.blockers is not None:\n",
    "            block_ok = all([i not in items for i in self.blockers])\n",
    "        else:\n",
    "            block_ok = True\n",
    "        return len(items) == len(set(items)) and block_ok\n",
    "    \n",
    "    \n",
    "    def _unpack_classes(self):\n",
    "        \"\"\"\n",
    "        Creates a dictionary where every harmonizing element \n",
    "        is mapped to its harmonic class; helps to optimize \n",
    "        the lookup of this information.\n",
    "        \n",
    "        Output type: dict\n",
    "        \"\"\"\n",
    "        items = self._undergoers()\n",
    "        unpacked = {}\n",
    "        for i in items:\n",
    "            for j in self.cl_members:\n",
    "                if i in j:\n",
    "                    unpacked[i] = self.cl_members[j]\n",
    "        return unpacked\n",
    "\n",
    "    \n",
    "    def _specify(self):\n",
    "        \"\"\"\n",
    "        Randomly initialize a specification from all given\n",
    "        harmonic datasets.\n",
    "        \n",
    "        Output type: list[char]\n",
    "        \"\"\"\n",
    "        return list(map(choice, self.cl_members.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of the data generated by AHG\n",
    "\n",
    "#### Parallel vowel and consonant harmonies\n",
    "Harmony of a class \"A\" that contains \"a\" and \"o\" and of a class \"B\" that contains \"b\" and \"p\". Linguistically, these are simultaneous and independent vowel and consonant harmonies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "h1 = Harmony(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate a sample of well-formed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bbbababbaa', 'aaapaaappp', 'poooppoopo', 'baaabaabbb', 'aapppaaapp']\n"
     ]
    }
   ],
   "source": [
    "print(h1.generate_words(n = 5, length = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmony with a transparent element\n",
    "\n",
    "Transparent, or irrelevant items that only introduce the long-distance effect in the dataset can be modeled by providing an extra harmonic class with just a single item in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = {(\"a\", \"o\"):\"A\", (\"x\"):\"X\"}\n",
    "l2 = {\"A\":(1, 2), \"X\":(2, 4)}\n",
    "h2 = Harmony(s2, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, us generate some well-formed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxxaaxxxxa', 'xxxxaxxxxa', 'xxxxaxxxxa', 'xxxoxxooxx', 'xxxoxxooxx']\n"
     ]
    }
   ],
   "source": [
    "print(h2.generate_words(n = 5, length = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel vowel and consonant harmonies with a blocking effect\n",
    "\n",
    "Harmony of a class \"A\" and of a class \"B\", where if \"t\" occurred, \"p\" cannot be observed anymore: class \"B\" changes its specification to \"p\". Namely, \"t\" is a blocker that only allows for \"p\" after itself.\n",
    "\n",
    "Additionally, clusters of the A-element consist usually from 1 to 3 elements, and clusters of the B-elements are 2 to 4 elements long. The probability of observing the blocker is $\\frac{1}{4}$ at every step of the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "l3 = {\"A\":(1, 3), \"B\":(2, 4)}\n",
    "b3 = {\"t\":\"p\"}\n",
    "p3 = 4\n",
    "h3 = Harmony(s3, l3, b3, p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate some well-formed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ppppoppoot', 'tooppttppt', 'pppaaapppp', 'tppaappaaa', 'aappaaappa']\n"
     ]
    }
   ],
   "source": [
    "print(h3.generate_words(n = 5, length = 10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Turkish generators and evaluators\n",
    "\n",
    "The following two functions are used to verify the well-formedness of generated Turkish or fake Turkish words:\n",
    "  * `backness_harmony` takes a string as input and tells if that strings is well-formed with respect to the rules of Turkish backness harmony;\n",
    "  * `rounding_harmony` does the same thing for the rounding harmony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backness_harmony(string):\n",
    "    \"\"\"\n",
    "    Tells if a string is well-formed according to rules\n",
    "    of Turkish backness harmony.\n",
    "    \"\"\"\n",
    "    front_class, back_class = \"Iaou\", \"ieOU\"\n",
    "    front, back = False, False\n",
    "    \n",
    "    for v in front_class + back_class:\n",
    "        if v in string:\n",
    "            front = True if v in front_class else front\n",
    "            back = True if v in back_class else back\n",
    "\n",
    "    return not (front and back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding_harmony(string):\n",
    "    \"\"\"\n",
    "    Tells if a string is well-formed according to rules\n",
    "    of Turkish rounding harmony.\n",
    "    \"\"\"\n",
    "    high, low, rounded = \"iIuU\", \"aeoO\", \"uUoO\"\n",
    "    \n",
    "    vowels = \"\".join([v for v in string if v in high + low])\n",
    "    if len(vowels) < 2:\n",
    "        return True\n",
    "    \n",
    "    ro = vowels[0] in rounded\n",
    "    \n",
    "    for v in vowels[1:]:\n",
    "        if v in low:\n",
    "            if v in rounded:\n",
    "                return False\n",
    "            ro = False\n",
    "        elif (ro and v not in rounded) or (not ro and v in rounded):\n",
    "            return False\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backness_and_rounding(string):\n",
    "    return backness_harmony(string) and rounding_harmony(string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, to generate simplified Turkish data, `turkish_word` and `generate_turkish_words` will generate a single word and a dataset, correspondingly.\n",
    "\n",
    "Their parameters are:\n",
    "* `length` is a desired length of the Turkish word;\n",
    "* `cond` is a choice of \"consonant\" that will be separating the vowels;\n",
    "* `vowel_cluster` is a tuple of integers representing minimal and maximal length of the vowel cluster;\n",
    "* `cons_cluster` is a tuple of integers representing minimal and maximal length of the consonantal cluster;\n",
    "* `n` (available for `generate_turkish` only) is the number of the examples that need to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turkish_word(length = 10, cons = \"x\", vowel_cluster = (1, 2),\n",
    "                          cons_cluster = (0, 3)):\n",
    "    \"\"\"\n",
    "    This generator generates fake Turkish words: namely, the words in which\n",
    "    the harmonic system and rules of Turkish are preserved, but all consonants\n",
    "    were substituted by a single given consonant.\n",
    "    \n",
    "    Arguments:\n",
    "    * length (int): a length of a word that needs to be generated;\n",
    "    * cons (str): a single character (or an empty string if only vowels\n",
    "                  need to be generated), a \"choice\" of the consonant \n",
    "                  that makes this harmony long-distant;\n",
    "    * vowel_cluster (tuple[int, int]): a tuple of integers representing\n",
    "                                       minimal and maximal length of\n",
    "                                       the vowel cluster;\n",
    "    * cons_cluster (tuple[int, int]): a tuple of integers representing\n",
    "                                      minimal and maximal length of\n",
    "                                      the consonantal cluster.\n",
    "                                      \n",
    "    Returns:\n",
    "    * str: a fake Turkish harmonic word, where all consonants are masked.\n",
    "    \"\"\"\n",
    "    if length < 1:\n",
    "        raise ValueError(\"Words cannot be so short.\")\n",
    "    \n",
    "    vowels = {\n",
    "        (True, True, True):\"u\",\n",
    "        (True, True, False):\"I\",\n",
    "        (True, False, True):\"o\",\n",
    "        (True, False, False):\"a\",\n",
    "        (False, True, True):\"U\",\n",
    "        (False, True, False):\"i\",\n",
    "        (False, False, True):\"O\",\n",
    "        (False, False, False):\"e\"\n",
    "    }\n",
    "    \n",
    "    backness = choice([True, False])\n",
    "    height = choice([True, False])\n",
    "    rounding = choice([True, False])\n",
    "    \n",
    "    specs = (backness, height, rounding)\n",
    "    word = \"\"\n",
    "    \n",
    "    if choice([0, 1]):\n",
    "            word += \"x\" * randint(*cons_cluster)\n",
    "            \n",
    "    while len(word) < length:\n",
    "        vc = vowels[specs] * randint(*vowel_cluster)\n",
    "        \n",
    "        # this part is neededd to avoid the word-initial *oo clusters\n",
    "        if len(vc) > 1 and not height and rounding:\n",
    "            rounding = False\n",
    "            vc = vc[0] + vowels[(backness, height, rounding)] * (len(vc) - 1)\n",
    "            \n",
    "        word += vc\n",
    "        word += \"x\" * randint(*cons_cluster)\n",
    "        \n",
    "        height = choice([True, False])\n",
    "        rounding = False if not height else rounding\n",
    "        specs = (backness, height, rounding)\n",
    "        \n",
    "    return word[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_turkish_words(n = 10, length = 10, cons = \"x\",\n",
    "                           vowel_cluster = (1, 2), cons_cluster = (1, 3)):\n",
    "    \"\"\"\n",
    "    This generator generates a list of fake Turkish words.\n",
    "    \n",
    "    Arguments:\n",
    "    * n (int): a number of strings that need to be generated;\n",
    "    ... for the rest of the arguments, see generate_turkish_word.\n",
    "    \n",
    "    Outputs:\n",
    "    * list: the list containing n fake Turkish words.\n",
    "    \"\"\"\n",
    "    return [turkish_word(length, cons, vowel_cluster, cons_cluster) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: other harmonic evaluators\n",
    "\n",
    "The function `harmonic_evaluator` below takes two arguments: `data` and `rule`. `data` is a list of words that need to be evaluated, and `rule` is the evaluation function for some concrete harmony. This function will be further used in order to evaluate the performance of the learners on the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def harmonic_evaluator(data, rule):\n",
    "    \"\"\"\n",
    "    Evaluates the provided data with respect to a given\n",
    "    rule of harmony.\n",
    "    \n",
    "    Arguments:\n",
    "    * data (list[str]): a list of strings tht need to be evaluated;\n",
    "    * rule (function): a function that evaluates a string according\n",
    "                       to some harmony.\n",
    "                       \n",
    "    Results:\n",
    "    * Prints the report that shows if the data follows the rule.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    incorrect = set()\n",
    "    #for w in progressBar(data, prefix = \"evaluating\"):# #\n",
    "    for w in data:# #\n",
    "        #correct = (correct + 1) if rule(w) else correct\n",
    "        if rule(w):\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect.add(w)\n",
    "        \n",
    "    ratio = (correct / len(data))\n",
    "    '''print(f\"Percentage of harmonic words: {int(ratio * 100)}%.\")\n",
    "    print(incorrect)\n",
    "    return incorrect''' # # #\n",
    "    return ratio # # #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finnish\n",
    "\n",
    "Finally, `front_harmony` defines a function that tells if a given string follows a rule of Finnish vowel harmony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def front_harmony(string):\n",
    "    \"\"\"\n",
    "    Tells if a string is well-formed according to rules\n",
    "    of Finnish backness harmony.\n",
    "    \"\"\"\n",
    "    front_class, back_class = \"AOy\", \"aou\"\n",
    "    front, back = False, False\n",
    "    \n",
    "    for v in front_class + back_class:\n",
    "        if v in string:\n",
    "            front = True if v in front_class else front\n",
    "            back = True if v in back_class else back\n",
    "\n",
    "    return not (front and back)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake harmony evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_harmony_no_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a single [a, o] harmony is well-formed.\n",
    "    \"\"\"\n",
    "    return not(\"a\" in string and \"o\" in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_harmony_with_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a single [a, o] harmony with a blocker f:a is well-formed.\n",
    "    \"\"\"\n",
    "    if \"f\" in string:\n",
    "        s1 = string[:string.index(\"f\")]\n",
    "        s2 = string[string.index(\"f\") + 1:]\n",
    "        return single_harmony_no_blockers(s1) and (not \"o\" in s2)\n",
    "    else:\n",
    "        return single_harmony_no_blockers(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_harmony(string, group = [\"a\", \"o\", \"u\", \"e\"]):\n",
    "    \"\"\"\n",
    "    Tells if a string contains only one out of four\n",
    "    (vowel) classes; check that at most one class\n",
    "    of vowels occurs within one word.\n",
    "    \n",
    "    Arguments:\n",
    "    * string (str): a string that needs to be verified;\n",
    "    * group (list[char]): the harmonic class.\n",
    "    \"\"\"\n",
    "    assert len(group) == 4\n",
    "    classes = 0\n",
    "    \n",
    "    for i in group:\n",
    "        classes = (classes + 1) if i in string else classes\n",
    "        \n",
    "    return classes in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_harmony_no_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a double [a, o] and [b, p] harmony is well-formed.\n",
    "    \"\"\"\n",
    "    vowels = not(\"a\" in string and \"o\" in string)\n",
    "    consonants = not(\"b\" in string and \"p\" in string)\n",
    "    return vowels and consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_harmony_with_blockers(string):\n",
    "    \"\"\"\n",
    "    Checks if a double [a, o] and [b, p] harmony with a blocker t:p\n",
    "    is well-formed.\n",
    "    \"\"\"\n",
    "    if \"a\" in string and \"o\" in string:\n",
    "        return False\n",
    "    \n",
    "    if \"t\" in string:\n",
    "        s1 = string[:string.index(\"t\")]\n",
    "        s2 = string[string.index(\"t\") + 1:]\n",
    "        return double_harmony_no_blockers(s1) and (\"b\" not in s2)\n",
    "    else:\n",
    "        return double_harmony_no_blockers(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Word-final devoicing generators and evaluators\n",
    "\n",
    "The functions `word_final_devoicing` and `generate_wfd` imitate the process of word-final devoicing.\n",
    "The former one generates a string or a pair of strings (UR -> SF) implementing that rule, and the latter one generates dataset consisting of ones.\n",
    "\n",
    "Their arguments are the following:\n",
    "* `sigma` is a list of symbols that can be used in the words;\n",
    "* `devoice` contains two tuples, where the first tuple represents voiced obstruents, and the second one stands for their voiceless counterparts;\n",
    "* `length` is the length of the intended words;\n",
    "* if `pairs` is True, (UG, SF) pairs will be returned, if False, only the surface forms;\n",
    "* `n` (available only for `generate_wfd`) is a number of strings or pairs that need to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_final_devoicing(sigma = (\"a\", \"b\", \"p\"), devoice = ((\"b\"), (\"p\")),\n",
    "                         length = 10, pairs = False):\n",
    "    \"\"\"\n",
    "    This function generates either a word grammatical with respect to a rule\n",
    "    of the word final devoicing, or a fake UG -> SF pair.\n",
    "    \n",
    "    Arguments: \n",
    "    * sigma (list[str]): a list of symbols that can be used in the words;\n",
    "    * devoice (tuple[tuple, tuple]): the first tuple represents voiced\n",
    "                                     obstruents, and the second one stands\n",
    "                                     for their voiceless counterparts;\n",
    "    * length (int): a length of the intended words;\n",
    "    * pairs (bool): if True, (UG, SF) pairs will be returned, if False, only\n",
    "                    the surface forms.\n",
    "                    \n",
    "    Outputs:\n",
    "    * str/tuple: a string or a tuple of strings (depending on the parameter \n",
    "                 `pairs`) representing the application of the word-final \n",
    "                 devoicing.\n",
    "    \"\"\"\n",
    "    if length < 1:\n",
    "        raise ValueError(\"The string has a very weird length.\")\n",
    "        \n",
    "    before, after = devoice\n",
    "    string = \"\".join([choice(sigma) for i in range(length)])\n",
    "    \n",
    "    if string[-1] not in before:\n",
    "        return (string, string) if pairs else string\n",
    "    \n",
    "    devoiced = string[:-1] + after[before.index(string[-1])]\n",
    "    return (string, devoiced) if pairs else devoiced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wfd(n = 10, sigma = (\"a\", \"b\", \"p\"), devoice = ((\"b\"), (\"p\")),\n",
    "                 length = 10, pairs = False):\n",
    "    \"\"\"\n",
    "    Generates a set of strings or pairs that satisfy the rule of\n",
    "    the word-final devoicing.\n",
    "    \n",
    "    Arguments:\n",
    "    * n (int): the number of strings that need to be generated;\n",
    "    ... for the rest of the arguments see word_final_devoicing.\n",
    "    \n",
    "    Outputs:\n",
    "    * list: a list of strings or tuples (depending on the parameter `pairs`)\n",
    "            representing the application of the word-final devoicing.\n",
    "    \"\"\"\n",
    "    return [word_final_devoicing(sigma, devoice, length, pairs) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `evaluate_wfd_words` evaluates words with respect to the rules of the word-final devoicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wfd_words(data, voiced = (\"b\")):\n",
    "    \"\"\"\n",
    "    Evaluates the provided words with respect to the rule \n",
    "    of the word-final devoicing.\n",
    "    \n",
    "    Arguments:\n",
    "    * data (list[str]): a list of strings tht need to be evaluated;\n",
    "    * voiced (tuple[char]): a list of voiced characters, i.e. those\n",
    "                            that cannot be word-final.\n",
    "                       \n",
    "    Results:\n",
    "    * Prints the report that shows if the data follows the ule.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    #for w in progressBar(data, prefix = \"evaluating\"):# #\n",
    "    for w in data:# #\n",
    "        \n",
    "        if not len(w):\n",
    "            correct += 1\n",
    "            continue\n",
    "            \n",
    "        correct = (correct + 1) if w[-1] not in voiced else correct\n",
    "        \n",
    "    ratio = (correct / len(data))\n",
    "    # # # print(f\"Percentage of well-formed words: {int(ratio * 100)}%.\")\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can generate some words or pairs of words representing the rule of the word-final devoicing, and then check if the evaluator considers that those datasets are well-formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_wfd_words(generate_wfd(n = 1000, pairs = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: UTP generator and evalurator\n",
    "\n",
    "The function `generate_tonal_pattern` takes a length of the string that needs to be generated, and returns a random string of raising (H) and falling (L) tones as output. `utp_tones` takes that string of tones as input, and rewrites it according to the UTP rules: no L tones are allowed in-between two H tones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tonal_pattern(length = 5):\n",
    "    \"\"\" Generates a random sequence of tones of a given length. \"\"\"\n",
    "    return \"\".join(choice([\"H\", \"L\"]) for i in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utp_tones(string):\n",
    "    \"\"\" Rewrites a tonal string with respect to the rules of UTP. \"\"\"\n",
    "    \n",
    "    if set(string) not in [{\"H\", \"L\"}, {\"H\"}, {\"L\"}, set(\"\")]:\n",
    "        print(string)\n",
    "        raise ValueError(\"Unexpected symbols in the tonal string!\")\n",
    "    if not (\"H\" in string and \"L\" in string):\n",
    "        return string\n",
    "    \n",
    "    first_h = string.find(\"H\")\n",
    "    last_h = len(string) - string[::-1].find(\"H\")\n",
    "    return string[:first_h] + \"H\" * (last_h - first_h) + string[last_h:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, `generate_utp_strings` generates strings of tones that are well-formed accroding to the rules of UTP. As before, `n` signifies the number of strings that need to be generated, and `length` is the length of those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_utp_strings(n = 10, length = 5):\n",
    "    \"\"\" Generates n strings of tones that follow UTP rules. \"\"\"\n",
    "    return [utp_tones(generate_tonal_pattern(length)) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `evaluate_utp_strings` and `evaluate_utp_pairs` calculate what is the percentage of the input data (strings or pairs of strings) is well-formed with respect to the rules of UTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_utp_strings(data):\n",
    "    \"\"\" Evaluates the correctness of if the given sample of tonal strings. \"\"\"\n",
    "    correct = 0\n",
    "    #for w in progressBar(data, prefix = \"evaluating\"):# #\n",
    "    for w in data:# #\n",
    "        correct = (correct + 1) if utp_tones(w) == w else correct\n",
    "        \n",
    "    ratio = (correct / len(data))\n",
    "    # # # print(f\"Percentage of well-formed tonal layers: {int(ratio * 100)}%.\")\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can verify the correctness of the generator using the evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_utp_strings(generate_utp_strings(n = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: First-last harmony generators and evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_last_UR(n = 10, length = 10):\n",
    "    \"\"\" Generates URs of first-last harmony words. \"\"\"\n",
    "    strings = []\n",
    "    for i in range(n):\n",
    "        new = choice([\"a\", \"o\"])\n",
    "        new += \"\".join([choice([\"a\", \"o\", \"x\"]) for j in range(length - 2)])\n",
    "        new += choice([\"a\", \"o\"])\n",
    "        strings.append(new)\n",
    "    return strings\n",
    "\n",
    "def first_last(string):\n",
    "    \"\"\" Makes the first and the last segment of the string the same. \"\"\"\n",
    "    return string[:-1] + string[0]\n",
    "\n",
    "def first_last_words(n = 10, length = 10):\n",
    "    \"\"\" Generates N first-last words. \"\"\"\n",
    "    return [first_last(w) for w in first_last_UR(n, length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_first_last_words(data):\n",
    "    \"\"\"\n",
    "    Evaluates the correctness of if the given sample\n",
    "    of first-last harmony (UR -> SF).\n",
    "    \"\"\"\n",
    "    newdata = [i for i in data if len(i) > 1]\n",
    "    if len(newdata) == 0 : return 1 # # #words with fewer than 2 characters pass by definition\n",
    "    correct = 0\n",
    "    #for w in progressBar(newdata, prefix = \"evaluating\"):# #\n",
    "    for w in newdata:# #\n",
    "        if w[0] == w[-1]:\n",
    "            correct += 1\n",
    "        \n",
    "    ratio = (correct / len(newdata))\n",
    "    # # #print(f\"Percentage of first-last harmonic words: {int(ratio * 100)}%.\")\n",
    "    return ratio # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sp_empty_word(alphabet, length = 5):\n",
    "    return \"\".join([choice(alphabet) for i in range(length)])\n",
    "\n",
    "def generate_sp_empty(alphabet, n = 10, length = 5):\n",
    "    return [generate_sp_empty_word(alphabet, length) for i in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing training samples for the experiments\n",
    "\n",
    "### Experiment 1: Word-final devoicing\n",
    "\n",
    "#### Artificial grammar: `toy_wfd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abppbaaabp', 'bppapbppaa', 'bppaapbapa', 'paaabpabap', 'bababpppbp', 'pabppapaba', 'apbbbababa', 'bpbapabbap', 'bappapabpp', 'pbaappaaap', 'bapbaaaaaa', 'pbbbbbpppp', 'aabbaabaap', 'pabppbbaap', 'baaapaaapp']\n"
     ]
    }
   ],
   "source": [
    "toy_wfd = generate_wfd(n = 1000)\n",
    "print(toy_wfd[:15])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw German data: `german_wfd`\n",
    "\n",
    "In German, orthography doesn't reflect the word-final devoicing. So first of all, all word-final /b/, /d/ and /g/ are rewritten as /p/, /t/ and /k/, correspondingly. Additionally, words with \"non-German\" characters are removed. The data comes from the [wordlist by enz](https://github.com/enz/german-wordlist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685618\n",
      "['Aa\\r', 'Aachener\\r', 'Aachenerin\\r', 'Aachenerinnen\\r', 'Aachenern\\r', 'Aacheners\\r', 'Aaden\\r', 'Aak\\r', 'Aake\\r', 'Aaken\\r'] ...\n"
     ]
    }
   ],
   "source": [
    "german_data = []\n",
    "with codecs.open('natural_data/german.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line != \"\":\n",
    "            german_data.append(line[:-1])\n",
    "            \n",
    "print(len(german_data))\n",
    "print(german_data[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of final /b/: 0\n",
      "Number of final /d/: 0\n",
      "Number of final /g/: 0\n"
     ]
    }
   ],
   "source": [
    "count_final_b = 0\n",
    "count_final_d = 0\n",
    "count_final_g = 0\n",
    "\n",
    "for i in german_data:\n",
    "    if i[-1] == \"b\":\n",
    "        count_final_b += 1\n",
    "    elif i[-1] == \"d\":\n",
    "        count_final_d += 1\n",
    "    elif i[-1] == \"g\":\n",
    "        count_final_g += 1\n",
    "        \n",
    "print(\"Number of final /b/:\", count_final_b) # 1599, or 0.2% words\n",
    "print(\"Number of final /d/:\", count_final_d) # 15294, or 2.2% words\n",
    "print(\"Number of final /g/:\", count_final_g) # 17098, or 2.4 % words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685147\n",
      "Clean dataset: ['aa\\r', 'aachener\\r', 'aachenerin\\r', 'aachenerinnen\\r', 'aachenern\\r', 'aacheners\\r', 'aaden\\r', 'aak\\r', 'aake\\r', 'aaken\\r', 'aakerbeere\\r', 'aakerbeeren\\r', 'aakes\\r', 'aaks\\r', 'aal\\r'] ...\n",
      "\n",
      "471\n",
      "Banned words: ['abbé\\r', 'abbés\\r', 'abrégé\\r', 'abrégés\\r', 'acheuléen\\r', 'acheuléens\\r', 'agrément\\r', 'agréments\\r', 'ampère\\r', 'ångström\\r'] ...\n"
     ]
    }
   ],
   "source": [
    "ban = ['à', 'á', 'â', 'å', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ñ', 'ó', 'õ', 'ú',\n",
    "       'û', 'č', 'ē', 'ī', 'ł', 'ō', 'œ', 'š', 'ū']\n",
    "\n",
    "german_wfd = []\n",
    "banned_words = []\n",
    "\n",
    "for w in german_data:\n",
    "    \n",
    "    word = w.lower()\n",
    "    \n",
    "    illegal = False\n",
    "    for b in ban:\n",
    "        if b in word:\n",
    "            banned_words.append(word)\n",
    "            illegal = True\n",
    "            break\n",
    "            \n",
    "    if illegal:\n",
    "        continue\n",
    "        \n",
    "    if word[-1] == \"b\":\n",
    "        word = word[:-1] + \"p\"\n",
    "    elif word[-1] == \"d\":\n",
    "        word = word[:-1] + \"t\"\n",
    "    elif word[-1] == \"g\":\n",
    "        word = word[:-1] + \"k\"\n",
    "        \n",
    "    german_wfd.append(word)\n",
    "\n",
    "print(len(german_wfd))\n",
    "print(\"Clean dataset:\", german_wfd[:15], \"...\\n\")\n",
    "\n",
    "print(len(banned_words))\n",
    "print(\"Banned words:\", banned_words[:10], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked German data: `german_wfd_masked`\n",
    "\n",
    "Now, let us substitute all segments that are not /p/, /t/, /k/, /b/, /d/, /g/ by \"a\".\n",
    "It will help further to try the learning algorithms on data that has less local dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685147\n",
      "Masked words: ['aakaabaaaaa', 'aakaabaaaaaa', 'aakaaa', 'aakaa', 'aaaa'] ...\n"
     ]
    }
   ],
   "source": [
    "german_wfd_masked = []\n",
    "for w in german_wfd:\n",
    "    new = \"\"\n",
    "    for s in w:\n",
    "        if s in [\"p\", \"t\", \"k\", \"b\", \"d\", \"g\"]:\n",
    "            new += s\n",
    "        else:\n",
    "            new += \"a\"\n",
    "    german_wfd_masked.append(new)\n",
    "german_data.append(\"\")\n",
    "    \n",
    "print(len(german_wfd_masked))\n",
    "print(\"Masked words:\", german_wfd_masked[10:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: One vowel harmony, no blockers\n",
    "\n",
    "#### Artificial grammar: `toy_vhnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxxooxxxxo', 'xxxxaaxxxa', 'ooxxooxxxx', 'ooxxooxxoo', 'aaxxxaaxxa', 'xxaxxxaaxx', 'axxxaxxxaa', 'ooxxxxoxxo', 'xxxxooxxxx', 'xxxxaaxxxx', 'xxxxaxxxxa', 'xxxxaaxxxx', 'ooxxxoxxoo', 'oxxooxxxxo', 'xxxxoxxoxx'] ...\n"
     ]
    }
   ],
   "source": [
    "ts2 = {(\"a\", \"o\"):\"A\", (\"x\"):\"X\"}\n",
    "tl2 = {\"A\":(1, 2), \"X\":(2, 4)}\n",
    "th2 = Harmony(ts2, tl2)\n",
    "toy_vhnb = th2.generate_words(n = 1000)\n",
    "print(toy_vhnb[:15], \"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Finnish data: `finnish_harmony`\n",
    "\n",
    "The next step is to have a dataset from a natural language that implements a single harmony.\n",
    "Here, we use Finnish data from [this link](https://github.com/douglasbuzatto/WordLists/blob/master/finnish-words.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287699\n",
      "['/* WP Hardening - 2016-06-19 19:09:32.261648 *', 'a', 'aa', 'aaa', 'aaaaaah', 'aaah', 'aaassa', 'aab', 'aaberge', 'aabraham'] ...\n"
     ]
    }
   ],
   "source": [
    "finnish_data = []\n",
    "with codecs.open('natural_data/finnish.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line != \"\":\n",
    "            finnish_data.append(line[:-2])\n",
    "            \n",
    "print(len(finnish_data))\n",
    "print(finnish_data[:10], \"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the unharmonic stems are filtered to clean the data. Apart from the digits and punctuations, words are also filtered that contain `}` that stands here in this dataset for Swedish `å`, and therefore is ill-defined in terms of the harmony. Then I rewrite `{` as `ä` and `|` as `ö` in order to normalize the spelling with respect to Turkish examples further. Finally, non-harmonic stems are filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250805\n",
      "Clean dataset: ['liitettAvAA', 'liitetyksi', 'liitetyt', 'liitetAAn', 'liitingin', 'liito', 'liitoille', 'liitoilleen', 'liitoissa', 'liitoista', 'liitoistaan', 'liitoksen', 'liitoksena', 'liitokset', 'liitoksi'] ...\n",
      "\n",
      "331\n",
      "Banned words: ['bl}baer', 'bl}field', 'bl}fieldin', 'bl}sar', 'bl}sare'] ...\n",
      "\n",
      "36563\n",
      "Non-harmonic words: ['aakkosjArjestykseen', 'aakkosjArjestyksessA', 'aaltoliikettA'] ...\n"
     ]
    }
   ],
   "source": [
    "ban = [' ', '*', '-', '.', '/', '0', '1', '2', '3', '4', '6', '8', '9', ':', '}']\n",
    "\n",
    "finnish_harmony = []\n",
    "banned_words = []\n",
    "non_harmonic = []\n",
    "\n",
    "for w in finnish_data:\n",
    "    \n",
    "    word = w.lower()\n",
    "    \n",
    "    illegal = False\n",
    "    for b in ban:\n",
    "        if b in word:\n",
    "            banned_words.append(word)\n",
    "            illegal = True\n",
    "            break\n",
    "            \n",
    "    if illegal:\n",
    "        continue\n",
    "    \n",
    "    word = word.replace(\"{\", \"A\")\n",
    "    word = word.replace(\"|\", \"O\")\n",
    "    if front_harmony(word):\n",
    "        finnish_harmony.append(word)\n",
    "    else:\n",
    "        non_harmonic.append(word)\n",
    "\n",
    "print(len(finnish_harmony))\n",
    "print(\"Clean dataset:\", finnish_harmony[105000:105015], \"...\\n\")\n",
    "\n",
    "print(len(banned_words))\n",
    "print(\"Banned words:\", banned_words[10:15], \"...\\n\")\n",
    "\n",
    "print(len(non_harmonic))\n",
    "print(\"Non-harmonic words:\", non_harmonic[:3], \"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Finnish data: `finnish_harmony_masked`\n",
    "\n",
    "Finally, all the transparent Finnish elements are masked in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250805\n",
      "Masked words: ['xauxaxxxxxoxxxxa', 'xauxaxxxxxoxxxxx', 'xauxaxxxxxxxax', 'xauxaxxxx', 'xauxaxxxxuxxxxx'] ...\n"
     ]
    }
   ],
   "source": [
    "finnish_harmony_masked = []\n",
    "for w in finnish_harmony:\n",
    "    new = \"\"\n",
    "    for s in w:\n",
    "        if s in [\"A\", \"O\", \"y\", \"a\", \"o\", \"u\"]:\n",
    "            new += s\n",
    "        else:\n",
    "            new += \"x\"\n",
    "    finnish_harmony_masked.append(new)\n",
    "    \n",
    "print(len(finnish_harmony_masked))\n",
    "print(\"Masked words:\", finnish_harmony_masked[170005:170010], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: One vowel harmony with blockers\n",
    "\n",
    "#### Artificial grammar: `toy_vhwb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxxooxofxf', 'xaxxxaaxxx', 'xaxffxxxaa', 'xoxxoxxxfa', 'oxxxoxooxx', 'xxxaaxaxxa', 'oofaxxaaff', 'xxxaafaaxa', 'axfaaxxaaf', 'xafaaxxaxx', 'xffaafxxxa', 'aaxxaxxfxx', 'fafxxaxxaa', 'aaxaaxxxaa', 'fxxxaxxxaf'] ...\n"
     ]
    }
   ],
   "source": [
    "harmonic_classes = {(\"a\", \"o\"):\"A\", (\"x\"):\"X\"}\n",
    "blockers = {\"f\":\"a\"}\n",
    "cluster_lengths = {\"A\":(1, 2), \"X\":(1, 3)}\n",
    "blocker_prob = 5\n",
    "h = Harmony(harmonic_classes, cluster_lengths, blockers, blocker_prob)\n",
    "toy_vhwb = h.generate_words(n = 1000)\n",
    "print(toy_vhwb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Two vowel harmonies, no blockers\n",
    "\n",
    "#### Artificial grammar: `toy_shnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oxxxooxxxo', 'xxoxxoxxxx', 'oxxxxoxxxx', 'xxxxoxxxoo', 'xxxxoxxxxo', 'xxxxaxxxaa', 'xxxxuxxxxu', 'exxxeexxee', 'exxxeexxxe', 'oxxxxoxxxx', 'xxxooxxxxo', 'ooxxxoxxxx', 'uxxuuxxxuu', 'xxuxxuuxxx', 'oxxooxxxoo'] ...\n"
     ]
    }
   ],
   "source": [
    "is2 = {(\"a\", \"e\", \"o\", \"u\"):\"A\", (\"x\"):\"X\"}\n",
    "il2 = {\"A\":(1, 2), \"X\":(2, 4)}\n",
    "ih2 = Harmony(is2, il2)\n",
    "toy_shnb = ih2.generate_words(n = 1000)\n",
    "print(toy_shnb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: Two vowel harmonies with vowel blockers\n",
    "\n",
    "#### Artificial grammar: `toy_mhwb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OxxUUxUU', 'xxaxxxaa', 'iixiiiix', 'Oexxxeex', 'xxaxxaxx', 'UUxxeeix', 'axIIaaxx', 'xOexxxex', 'xxxuuuxx', 'aaxaxaaI', 'xxUUUxxx', 'xxuuxuxa', 'aaaxaxxI', 'axxIIaaI', 'xxOUxxee'] ...\n"
     ]
    }
   ],
   "source": [
    "toy_mhwb = generate_turkish_words(n = 5000, length = 8, cons_cluster = (0, 3))\n",
    "toy_mhwb.extend(generate_turkish_words(n = 5000, length = 6, cons_cluster = (0, 3)))# # #Changed 3 to 4 on all cons_cluster parameters\n",
    "toy_mhwb.extend(generate_turkish_words(n = 5000, length = 4, cons_cluster = (0, 3)))\n",
    "print(toy_mhwb[:15], \"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Turkish data: `turkish_harmony`\n",
    "\n",
    "The following is a dataset of Turkish harmony from [here](http://www.swarthmore.edu/SocSci/harmony/public_html/dummyresults.html). Non-native Turkish words are removed, and also the ones that do not follow the rules of backness and rounding harmony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890\n",
      "['ey-', 'gadr-', 'eG-', 'kesr-', 'tard-', 'keyf-', 'kos-', 'ledel-', 'garb-', 'ekto-', 'ekz-', 'fasl-', 'elektrik-Gi', 'elektro-', 'terkib-', 'abs-', 'lem-', 'koyn-', 'sUlUUk-u', 'hacr-', 'hacz-', 'hadd-', 'tesb-', 'li-', 'kIral-', 'hafid-', 'kriyo-', 'kriz-', 'hakk-', 'kIKr-'] ...\n",
      "\n",
      "10545\n",
      "['kesad', 'konukomKu', 'kesafet', 'somaki', 'kesan', 'kesat', 'lagemut', 'lagos', 'fuzuli', 'eyalet', 'rufai', 'ruhulkudUs', 'gaavur', 'gaavurca', 'gabardin', 'gabari', 'gabavet', 'kesedar', 'somye', 'konvansiyon', 'kooperatif', 'koordinasyon', 'sondeyiK', 'gabi', 'gabin', 'eylUUl', 'gabro', 'eylUl', 'eytam', 'gaco'] ...\n",
      "\n",
      "14434\n",
      "['som', 'lafazan', 'konuk', 'kekti', 'lafzan', 'konukCu', 'somak', 'laGar', 'laGIm', 'konulmak', 'somruk', 'laGIv', 'konum', 'somun', 'kesb', 'somurdanmak', 'konuk', 'somurmak', 'romanyalI', 'ru', 'ey', 'fuzul', 'gaah', 'eyer', 'gaasIb', 'eyercilik', 'rum', 'eyi', 'rumca', 'eyice'] ...\n"
     ]
    }
   ],
   "source": [
    "banned = []\n",
    "non_harmonic = []\n",
    "turkish_harmony = []\n",
    "\n",
    "with codecs.open('natural_data/turkish.txt', encoding='utf-8') as f:\n",
    "    \n",
    "    ban = [\"!\", \"-\", \"w\", \"x\", \"A\"]\n",
    "    for line in f:\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        w = line[:-2]\n",
    "        \n",
    "        if any([(i in w) for i in ban]):\n",
    "            banned.append(w)\n",
    "            continue\n",
    "            \n",
    "        if backness_harmony(w) and rounding_harmony(w):\n",
    "            w = w.replace(\"K\", \"k\")\n",
    "            turkish_harmony.append(w)\n",
    "        else:\n",
    "            non_harmonic.append(w)\n",
    "            \n",
    "print(len(banned))\n",
    "print(banned[:30], \"...\\n\")\n",
    "\n",
    "print(len(non_harmonic))\n",
    "print(non_harmonic[:30], \"...\\n\")\n",
    "            \n",
    "print(len(turkish_harmony))\n",
    "print(turkish_harmony[:30], \"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Turkish data: `turkish_harmony_masked`\n",
    "Then, the Turkish harmonic data is simplified by masking all non-vowels as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14434\n",
      "Masked words: ['xOxxex', 'xaxxaxxIxxax', 'xOxxUx', 'xaxxaxIx', 'xOxex'] ...\n"
     ]
    }
   ],
   "source": [
    "turkish_harmony_masked = []\n",
    "for w in turkish_harmony:\n",
    "    new = \"\"\n",
    "    for s in w:\n",
    "        if s in \"iIuUaeoO\":\n",
    "            new += s\n",
    "        else:\n",
    "            new += \"x\"\n",
    "    turkish_harmony_masked.append(new)\n",
    "    \n",
    "print(len(turkish_harmony_masked))\n",
    "print(\"Masked words:\", turkish_harmony_masked[12005:12010], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: Vowel harmony and consonant harmony, no blockers\n",
    "\n",
    "#### Artificial grammar: `toy_dhnb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pppoopoppp', 'babbbaabbb', 'oobbbobbbo', 'ooobbbobbo', 'aaapappapp', 'bbobbbobbb', 'bbbabbaaab', 'paapaaappp', 'aaapaapaaa', 'opooppoppo', 'oboobboobb', 'aaabaabbaa', 'ooppoooppp', 'pppaappaaa', 'oobbbooobo'] ...\n"
     ]
    }
   ],
   "source": [
    "iss = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "ihs = Harmony(iss)\n",
    "toy_dhnb = ihs.generate_words(n = 1000)\n",
    "print(toy_dhnb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: Vowel harmony and consonant harmony with blockers\n",
    "\n",
    "#### Artificial grammar: `toy_dhwb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['topopoppoo', 'pototopopp', 'ppapappapp', 'popptpoppo', 'pootttpoot', 'babbaabbab', 'oboobttopo', 'ooptppoopp', 'ttppappapp', 'abaattaapa', 'ootppooppt', 'aabtaattpp', 'otppooppoo', 'atpattppta', 'oppoppoopp'] ...\n"
     ]
    }
   ],
   "source": [
    "aa = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n",
    "bb = {\"A\":(1, 2), \"B\":(1, 2)}\n",
    "cc = {\"t\":\"p\"}\n",
    "dd = 5\n",
    "hmm = Harmony(aa, bb, cc, dd)\n",
    "toy_dhwb = hmm.generate_words(n = 5000)\n",
    "print(toy_dhwb[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8: Tonal plateauing\n",
    "#### Artificial grammar: `toy_utp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HHHHH', 'HHHLL', 'HHHHL', 'HLLLL', 'LLLLH', 'HHHHL', 'HHHHH', 'HHHHH', 'HLLLL', 'HHLLL', 'HHHHH', 'LLLHH', 'HHHHL', 'LHHHH', 'HHHHH'] ...\n"
     ]
    }
   ],
   "source": [
    "toy_utp = generate_utp_strings(n = 1000)\n",
    "print(toy_utp[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 9: First-last harmony\n",
    "#### Artificial grammar: `first_last_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oaooaaoaao', 'aaxxoaoxoa', 'oxaoxxxxxo', 'aaooxxoxoa', 'oaoaaooaoo', 'oxooooaaoo', 'axoxooxxxa', 'oooooxxaoo', 'oooooooxoo', 'oaoooxoaxo', 'oxoxooaooo', 'oxxoaoaxxo', 'oaxoxaxoao', 'aoaaxaoxoa', 'ooaaxooooo'] ...\n"
     ]
    }
   ],
   "source": [
    "first_last_data = first_last_words(n = 5000)\n",
    "print(first_last_data[:15], \"...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 10: Multi-tier Input-Sensitive Harmony\n",
    "\n",
    "The trigger for a long-distance assimilation depends on a local context. For example, `e` immediately before `x` prohibits `a` anywhere further after `e` in the string. Then `eaaxaae` and `axaexeeexx` are good, while `exxae` is not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparing a class to encode input sensitive rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSRule(object):\n",
    "    \"\"\" A generic template for a input-sensititve rule. \n",
    "    \n",
    "    * symbols (tuple): list of tier symbols relevant for the generalization;\n",
    "    * target (str): a target character context of which is important;\n",
    "    * right_context (str): a context in which a target character\n",
    "                           is projected on the tier;\n",
    "    * can_follow (tuple): a list of tier symbols that are allowed after\n",
    "                         the target character is projected.\n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, target, right_context, can_follow):\n",
    "        self.symbols = symbols\n",
    "        self.target = target\n",
    "        self.right_context = right_context\n",
    "        self.can_follow = can_follow\n",
    "\n",
    "    def is_grammatical(self, string):\n",
    "        \"\"\" Checks if the given form follows a rule that is encoded.\n",
    "        \n",
    "        * string (str): a string well-formedness of which needs to be checked.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get rid of all irrelevant symbols (not symbols and contexts)\n",
    "        string = \"\".join([i for i in string if i in list(self.symbols) + [self.right_context]])\n",
    "        \n",
    "        # construct a tier of that strings\n",
    "        tier = \"\"\n",
    "        for i in range(len(string)):\n",
    "            if string[i] in self.symbols:\n",
    "                if string[i] == self.target and i < len(string) - 1 and\\\n",
    "                    string[i + 1] == self.right_context:\n",
    "                    tier += self.target\n",
    "                elif string[i] != self.target:\n",
    "                    tier += string[i]\n",
    "\n",
    "        # check if that tier is well-formed\n",
    "        for t in range(len(tier)):\n",
    "            if tier[t] == self.target and t < len(tier) - 1 and\\\n",
    "                tier[t + 1] not in self.can_follow:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Writing a generator of a sequence grammatical wrt the rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rule_sequence(rule, length = 7, grammatical = True):\n",
    "    \"\"\" This function generates a sequence of symbols (un)grammatical \n",
    "        with respect to the given rule.\n",
    "        \n",
    "    * rule (SSRule): a rule describing an input sensitive dependency;\n",
    "    * length (int): length of the generated sequence;\n",
    "    * grammatical (bool): produces correct form when set to True, and \n",
    "                          makes a mistake when set to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the generation of the well-formed sequence is done by a simplistic FSA\n",
    "    sequence = \"\"\n",
    "    state = 0\n",
    "    for i in range(length):\n",
    "        \n",
    "        # State 0: the target was not observed\n",
    "        if state == 0:\n",
    "            sequence += choice(list(rule.symbols) + [rule.right_context])\n",
    "            if sequence[-1] == rule.target:\n",
    "                state = 1\n",
    "                \n",
    "        # State 1: the target was observed\n",
    "        elif state == 1:\n",
    "            sequence += choice(list(rule.symbols) + [rule.right_context])\n",
    "            if sequence[-1] == rule.right_context:\n",
    "                state = 2\n",
    "            elif sequence[-1] != rule.target:\n",
    "                state = 0\n",
    "                \n",
    "        # State 2: the right context was observed\n",
    "        elif state == 2:\n",
    "            sequence += choice(list(rule.can_follow) + [rule.right_context])\n",
    "                \n",
    "    # if the ungrammatical form is needed, a violating sequence is generated\n",
    "    # and inserted into a random position within the sequence\n",
    "    if not grammatical:\n",
    "        violate = rule.target + rule.right_context +\\\n",
    "            choice([i for i in list(rule.symbols) if i not in rule.can_follow])\n",
    "        index_violate = choice(range(length - 3))\n",
    "        sequence = sequence[:index_violate] + violate + sequence[index_violate + 3:]\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intertwine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intertwine(str1, str2, r = (0, 3)):\n",
    "    \"\"\" Intertwines two strings: str1 and str2. At every step, it takes\n",
    "    some characters from one string, and then some characters from another.\n",
    "    oxxooxa\n",
    "    * str1 (str): the first string;\n",
    "    * str2 (str): the second string;\n",
    "    * r (tuple[int, int]): min and max+1 symbols to be taken.\n",
    "    \"\"\"\n",
    "    new_string = \"\"\n",
    "    current = choice([1, 2])\n",
    "    while str1 or str2:\n",
    "        if current == 1:\n",
    "            cut = choice(range(r[0], r[1]))\n",
    "            if len(str1) < cut:\n",
    "                new = str1[:]\n",
    "            else:\n",
    "                new = str1[:cut]\n",
    "            new_string += new\n",
    "            str1 = str1[len(new):]\n",
    "            current = 2\n",
    "        elif current == 2:\n",
    "            cut = choice(range(r[0], r[1]))\n",
    "            if len(str2) < cut:\n",
    "                new = str2[:]\n",
    "            else:\n",
    "                new = str2[:cut]\n",
    "            new_string += new\n",
    "            str2 = str2[len(new):]\n",
    "            current = 1\n",
    "    return new_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generator for the ITSL harmony\n",
    "A single locally-driven long-distance assimilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itsl_harmony_generate(n = 10, length = 10, grammatical = True,\n",
    "                       rule_1 = None):\n",
    "    \"\"\" Generates a collection words following the given rules of \n",
    "    input-sensitive dependencies that involve a single tier.\n",
    "    \n",
    "    * n (int): number of strings that need to be generated;\n",
    "    * length (int): length of every one of the generated strings;\n",
    "    * grammatical (bool): if set to True, the correctly harmonizing\n",
    "                          forms are generated, and if set to False,\n",
    "                          the disharmonic forms are produced;\n",
    "    * rule_1 (SSRule): the first rule describing a long-distant input-\n",
    "                      sensitive dependency.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the first rule\n",
    "    if rule_1 == None:\n",
    "        rule_1 = SSRule(symbols = (\"o\", \"e\", \"a\"), target = \"o\",\\\n",
    "                        right_context = \"x\", can_follow = (\"a\", \"o\"))\n",
    "    strings = []\n",
    "    for i in range(n):\n",
    "        string = generate_rule_sequence(rule_1, length)\n",
    "        if not grammatical:\n",
    "            string = generate_rule_sequence(rule_1, length, grammatical = False)\n",
    "        strings.append(string)\n",
    "    return strings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generator for the MITSL harmony\n",
    "Two locally-driven long-distance assimilations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mitsl_harmony_generate(n = 10, length = 10, grammatical = True,\n",
    "                       rule_1 = None, rule_2 = None):\n",
    "    \"\"\" Generates a collection words following the given rules of the \n",
    "    input sensitive dependencies that involve several tiers.\n",
    "    \n",
    "    * n (int): number of strings that need to be generated;\n",
    "    * length (int): length of every one of the generated strings;\n",
    "    * grammatical (bool): if set to True, the correctly harmonizing\n",
    "                          forms are generated, and if set to False,\n",
    "                          the disharmonic forms are produced;\n",
    "    * rule_1 (SSRule): the first rule describing a long-distant input-\n",
    "                      sensitive dependency;\n",
    "    * rule_2 (SSRule): the second rule describing a long-distant input-\n",
    "                      sensitive dependency.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set both rules\n",
    "    if rule_1 == None:\n",
    "        rule_1 = SSRule(symbols = (\"o\", \"e\", \"a\"), target = \"o\",\\\n",
    "                        right_context = \"x\", can_follow = (\"a\", \"o\"))\n",
    "    if rule_2 == None:\n",
    "        rule_2 = SSRule(symbols = (\"b\", \"p\", \"d\"), target = \"b\",\\\n",
    "                        right_context = \"y\", can_follow = (\"b\", \"p\"))\n",
    "    \n",
    "    strings = []\n",
    "    for i in range(n):\n",
    "        # generate two tiers independently, and then intertwine them\n",
    "        # WARNING: the tier alphabets of the two rules cannot overlap\n",
    "        #          (required by both learner and generator)\n",
    "        len_part_1 = length // 2\n",
    "        len_part_2 = length - len_part_1\n",
    "\n",
    "        part_1 = generate_rule_sequence(rule_1, len_part_1)\n",
    "        part_2 = generate_rule_sequence(rule_2, len_part_2)\n",
    "\n",
    "        if not grammatical:\n",
    "            mistake = choice([\"R1\", \"R2\", \"both\"])\n",
    "            if mistake == \"R1\":\n",
    "                part_1 = generate_rule_sequence(rule_1, len_part_1, grammatical = False)\n",
    "            elif mistake == \"R2\":\n",
    "                part_2 = generate_rule_sequence(rule_2, len_part_2, grammatical = False)\n",
    "            else:\n",
    "                part_1 = generate_rule_sequence(rule_1, len_part_1, grammatical = False)\n",
    "                part_2 = generate_rule_sequence(rule_2, len_part_2, grammatical = False)\n",
    "\n",
    "        # intertwining the two generated sequences\n",
    "        new_string = intertwine(part_1, part_2)\n",
    "        strings.append(new_string)\n",
    "        \n",
    "    return strings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools: collecting data generators for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_harmony(kind=\"first-last\", length=range(2, 7), number=1000):\n",
    "    \"\"\"\n",
    "    Generates a harmony based on 3 parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    * kind (str): type of the harmony, choices:\n",
    "        \"first-last\", \"double\", \"assimilation-one\", \"assimilation-two\"\n",
    "    * length (range): a range of lengths of the intended strings\n",
    "    * number (int): a number of strings to be generated\n",
    "    \n",
    "    Outputs:\n",
    "    * list: a collection of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # preparing data for easy generation\n",
    "    lennum = {r:number // len(length) for r in length}\n",
    "    hmap = {\"assimilation-two\" : mitsl_harmony_generate}\n",
    "    \n",
    "    # generating the data\n",
    "    data = [i for l in lennum for i in hmap[kind](lennum[l], l)]\n",
    "    \n",
    "    # annotate the data with start- and end-markers >> and <<\n",
    "    # # #return list(map(lambda string : \">>\" + string + \"<<\", data)) # # #Annotation is not needed, SigmaPie-style learners do this on their own\n",
    "    return list(map(lambda string : string, data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mitsl_word(rules: list[SSRule], string: str):\n",
    "    for rule in rules:\n",
    "        if rule.target + rule.right_context not in string:\n",
    "            continue\n",
    "        '''chars = set()\n",
    "        chars.update(rule.symbols)\n",
    "        chars.update(rule.target)\n",
    "        chars.update(rule.right_context)\n",
    "        chars.update(rule.can_follow)\n",
    "        tier = re.sub(f'[^{\"\".join(chars)}]', '', string)#replace all irrelevant characters'''\n",
    "        tier = re.sub(f'[^{\"\".join(rule.symbols)}]', '', re.sub(f'.*?{rule.target + rule.right_context}', '', string))\n",
    "        if any(s not in rule.can_follow for s in tier):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def evaluate_mitsl_words(strings):\n",
    "    rule_1 = SSRule(symbols = (\"o\", \"e\", \"a\"), target = \"o\",\\\n",
    "                        right_context = \"x\", can_follow = (\"a\", \"o\"))\n",
    "    rule_2 = SSRule(symbols = (\"b\", \"p\", \"d\"), target = \"b\",\\\n",
    "                        right_context = \"y\", can_follow = (\"b\", \"p\"))\n",
    "    \n",
    "    correct = 0\n",
    "    incorrect = set()\n",
    "    for string in strings:\n",
    "        if evaluate_mitsl_word([rule_1, rule_2], string):\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect.add(string)\n",
    "    ratio = (correct / len(strings))\n",
    "    '''print(f\"Percentage of harmonic words: {int(ratio * 100)}%.\")\n",
    "    print(incorrect)''' # # #\n",
    "    return ratio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assim_two = generate_harmony(\"assimilation-two\", range(2, 8), number = 10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick reference to the datasets\n",
    "\n",
    "* **Word-final devoicing**\n",
    "  * `toy_wfd` (1,000 words)\n",
    "  * `german_wfd` (685,147 words)\n",
    "  * `german_wfd_masked` (131,938 words)\n",
    "  \n",
    "  \n",
    "* **Single vowel harmony, no blockers**\n",
    "  * `toy_vhnb` (1,000 words)\n",
    "  * `finnish_harmony` (250,805 words)\n",
    "  * `finnish_harmony_masked` (77,108 words)\n",
    "  \n",
    "  \n",
    "* **Single vowel harmony with blockers**\n",
    "  * `toy_vhwb` (1,000 words)\n",
    "  \n",
    "    \n",
    "* **Two vowel harmonies, no blockers**\n",
    "  * `toy_shnb` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **Two vowel harmonies with vowel blockers**\n",
    "  * `toy_mhwb` (15,000 words)\n",
    "  * `turkish_harmony` (14,434 words)\n",
    "  * `turkish_harmony_masked` (1,769 words)\n",
    "  \n",
    "  \n",
    "* **Vowel harmony and consonant harmony, no blockers**\n",
    "  * `toy_dhnb` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **Vowel harmony and consonant harmony with blockers**\n",
    "  * `toy_dhwb` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **Unbounded tonal plateauing**\n",
    "  * `toy_utp` (1,000 words)\n",
    "  \n",
    "  \n",
    "* **First-last harmony**\n",
    "  * `first_last_data` (5,000 words)  \n",
    "  \n",
    "\n",
    "* **First-last harmony**\n",
    "  * `assim_two` (10,000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "def save(i, sample, acc, sigstar, g, wrong):\n",
    "    '''\n",
    "    This is a simple helper method to pickle a bunch of experiment outputs in case the jupyter kernel crashes\n",
    "    '''\n",
    "    os.system(f'rm -rf {i}'); os.system(f'mkdir {i}')\n",
    "    pickle.dump(sample, open(f'{i}/{\"sample\"}', 'wb'))\n",
    "    pickle.dump(list(sigstar), open(f'{i}/{\"sigstar\"}', 'wb'))\n",
    "    pickle.dump(g, open(f'{i}/{\"g\"}', 'wb'))\n",
    "    pickle.dump(list(wrong), open(f'{i}/{\"wrong\"}', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MITSL experiments\n",
    "\n",
    "## Experiment 1: Word-final devoicing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl1\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_wfd\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_wfd_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "acc1 = lambda w : evaluate_wfd_words([w])           #create an acceptor function which the grammar should learn to mimic, using the above acceptors from Aksënova (2020)\n",
    "sigstar1 = list(star(set(''.join(toy_wfd)), 5))     #generate all strings made from this language's alphabet up to length 5\n",
    "\n",
    "g1 = learn(toy_wfd + [\"\"], k=2,m=2)                 #learn a k=2,m=2 grammar from this sample\n",
    "\n",
    "wrong1 = wrong(g1, acc1, sigstar1)                  #find all strings in sigma* where this grammar gives the wrong answer \n",
    "\n",
    "save(1, toy_wfd, acc1, sigstar1, g1, wrong1)        #save the results to be used later (see the section after experiment 16)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German simplified word-final devoicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\'\\'this = \"mitsl2\"\\nglobals()[this] = MITSL(polar = \"n\")\\nglobals()[this].data = german_wfd_masked\\nglobals()[this].data.append(\"\") # added to eliminate *>< on all tiers\\nglobals()[this].extract_alphabet()\\nglobals()[this].learn()\\nglobals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\\nevaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\\nprint(\"--------------------------\")\\nprint(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\\nprint(\"--------------------------\")\\nprint(\"Size of the grammar:\", len(globals()[this].grammar))\\nprint(\"--------------------------\")\\nprint(\"Grammars:\", globals()[this].grammar)\\'\\'\\'\\n\\n\\nacc2 = lambda w : evaluate_wfd_words([w])#acceptor\\nsigstar2 = star(set(\\'\\'.join(german_wfd_masked)), 5)\\n\\ng2 = learn(german_wfd_masked + [\"\"], k=2,m=2)\\n\\nwrong2 = wrong(g2, acc2, sigstar2)\\n\\nprint(len(wrong2))\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"'''this = \"mitsl2\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = german_wfd_masked\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_wfd_words(globals()[this+\"_sample\"], voiced = (\"b\", \"d\", \"g\"))\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc2 = lambda w : evaluate_wfd_words([w])#acceptor\n",
    "sigstar2 = star(set(''.join(german_wfd_masked)), 5)\n",
    "\n",
    "g2 = learn(german_wfd_masked + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong2 = wrong(g2, acc2, sigstar2)\n",
    "\n",
    "print(len(wrong2))\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Single vowel harmony without blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl4\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_vhnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc4 = lambda w : harmonic_evaluator([w], single_harmony_no_blockers)#acceptor\n",
    "sigstar4 = list(star(set(''.join(toy_vhnb)), 5))\n",
    "\n",
    "g4 = learn(toy_vhnb + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong4 = wrong(g4, acc4, sigstar4)\n",
    "\n",
    "save(4, toy_vhnb, acc4, sigstar4, g4, wrong4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Finnish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\'\\'this = \"mitsl5\"\\nglobals()[this] = MITSL(polar = \"n\")\\nglobals()[this].data = finnish_harmony_masked\\nglobals()[this].data.append(\"\") # added to eliminate *>< on all tiers\\nglobals()[this].extract_alphabet()\\nglobals()[this].learn()\\nglobals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\\nglobals()[this+\"_sample\"] = pickle.load(open(f\\'results_/{this}/sample\\', \\'rb\\'))\\nharmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\\nprint(\"--------------------------\")\\nprint(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\\nprint(\"--------------------------\")\\nprint(\"Size of the grammar:\", len(globals()[this].grammar))\\nprint(\"--------------------------\")\\nprint(\"Grammars:\", globals()[this].grammar)\\'\\'\\'\\n\\n\\nacc5 = lambda w : harmonic_evaluator([w], front_harmony)#acceptor\\nsigstar5 = star(set(\\'\\'.join(finnish_harmony_masked)), 5)\\n\\ng5 = learn(finnish_harmony_masked + [\"\"], k=2,m=2)\\n\\nwrong5 = wrong(g5, acc5, sigstar5)\\n\\nprint(len(wrong5))'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"'''this = \"mitsl5\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = finnish_harmony_masked\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "globals()[this+\"_sample\"] = pickle.load(open(f'results_/{this}/sample', 'rb'))\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], front_harmony)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc5 = lambda w : harmonic_evaluator([w], front_harmony)#acceptor\n",
    "sigstar5 = star(set(''.join(finnish_harmony_masked)), 5)\n",
    "\n",
    "g5 = learn(finnish_harmony_masked + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong5 = wrong(g5, acc5, sigstar5)\n",
    "\n",
    "print(len(wrong5))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Single vowel harmony with blockers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl7\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_vhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], single_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc7 = lambda w : harmonic_evaluator([w], single_harmony_with_blockers)#acceptor\n",
    "sigstar7 = list(star(set(''.join(toy_vhwb)), 5))\n",
    "\n",
    "g7 = learn(toy_vhwb + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong7 = wrong(g7, acc7, sigstar7)\n",
    "\n",
    "save(7, toy_vhwb, acc7, sigstar7, g7, wrong7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Two vowel harmonies, no blockers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl8\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_shnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc8 = lambda w : harmonic_evaluator([w], double_harmony)#acceptor\n",
    "sigstar8 = list(star(set(''.join(toy_shnb)), 5))\n",
    "\n",
    "g8 = learn(toy_shnb + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong8 = wrong(g8, acc8, sigstar8)\n",
    "\n",
    "save(8, toy_shnb, acc8, sigstar8, g8, wrong8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: Two vowel harmonies with vowel blockers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl9\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_mhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc9 = lambda w : harmonic_evaluator([w], backness_and_rounding)#acceptor\n",
    "sigstar9 = list(star(set(''.join(toy_mhwb)), 5))\n",
    "\n",
    "g9 = learn(toy_mhwb + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong9 = wrong(g9, acc9, sigstar9)\n",
    "\n",
    "save(9, toy_mhwb, acc9, sigstar9, g9, wrong9)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Turkish harmony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\'\\'this = \"mitsl10\"\\nglobals()[this] = MITSL(polar = \"n\")\\nglobals()[this].data = turkish_harmony_masked\\nglobals()[this].data.append(\"\") # added to eliminate *>< on all tiers\\nglobals()[this].extract_alphabet()\\nglobals()[this].learn()\\nglobals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\\nharmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\\nprint(\"--------------------------\")\\nprint(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\\nprint(\"--------------------------\")\\nprint(\"Size of the grammar:\", len(globals()[this].grammar))\\nprint(\"--------------------------\")\\nprint(\"Grammars:\", globals()[this].grammar)\\'\\'\\'\\n\\n\\nacc10 = lambda w : harmonic_evaluator([w], backness_and_rounding)#acceptor\\nsigstar10 = star(set(\\'\\'.join(turkish_harmony_masked)), 5)\\n\\ng10 = learn(turkish_harmony_masked + [\"\"], k=2,m=2)\\n\\nwrong10 = wrong(g10, acc10, sigstar10)\\n\\nprint(len(wrong10))'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"'''this = \"mitsl10\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = turkish_harmony_masked\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], backness_and_rounding)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc10 = lambda w : harmonic_evaluator([w], backness_and_rounding)#acceptor\n",
    "sigstar10 = star(set(''.join(turkish_harmony_masked)), 5)\n",
    "\n",
    "g10 = learn(turkish_harmony_masked + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong10 = wrong(g10, acc10, sigstar10)\n",
    "\n",
    "print(len(wrong10))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6: Vowel harmony and consonant harmony, no blockers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl12\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_dhnb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_no_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc12 = lambda w : harmonic_evaluator([w], double_harmony_no_blockers)#acceptor\n",
    "sigstar12 = list(star(set(''.join(toy_dhnb)), 5))\n",
    "\n",
    "g12 = learn(toy_dhnb + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong12 = wrong(g12, acc12, sigstar12)\n",
    "\n",
    "save(12, toy_dhnb, acc12, sigstar12, g12, wrong12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7: Vowel harmony and consonant harmony with blockers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl13\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_dhwb\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "harmonic_evaluator(globals()[this+\"_sample\"], double_harmony_with_blockers)\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc13 = lambda w : harmonic_evaluator([w], double_harmony_with_blockers)#acceptor\n",
    "sigstar13 = list(star(set(''.join(toy_dhwb)), 5))\n",
    "\n",
    "g13 = learn(toy_dhwb + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong13 = wrong(g13, acc13, sigstar13)\n",
    "\n",
    "save(13, toy_dhwb, acc13, sigstar13, g13, wrong13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8: Unbounded tonal plateauing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl14\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = toy_utp\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_utp_strings(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"First 30 restrictions:\", globals()[this].grammar[:30])'''\n",
    "\n",
    "\n",
    "acc14 = lambda w : evaluate_utp_strings([w])#acceptor\n",
    "sigstar14 = list(star(set(''.join(toy_utp)), 5))\n",
    "\n",
    "g14 = learn(toy_utp + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong14 = wrong(g14, acc14, sigstar14)\n",
    "\n",
    "save(14, toy_utp, acc14, sigstar14, g14, wrong14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9: First-last harmony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl15\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = first_last_data\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_first_last_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc15 = lambda w : evaluate_first_last_words([w])#acceptor\n",
    "sigstar15 = list(star(set(''.join(first_last_data)), 5))\n",
    "\n",
    "g15 = learn(first_last_data + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong15 = wrong(g15, acc15, sigstar15)\n",
    "\n",
    "save(15, first_last_data, acc15, sigstar15, g15, wrong15)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 10: Two locally-driven long-distance assimilations (ITSL restrictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this = \"mitsl16\"\n",
    "globals()[this] = MITSL(polar = \"n\")\n",
    "globals()[this].data = assim_two\n",
    "globals()[this].data.append(\"\") # added to eliminate *>< on all tiers\n",
    "globals()[this].extract_alphabet()\n",
    "globals()[this].learn()\n",
    "globals()[this+\"_sample\"] = globals()[this].generate_sample(n = 1000)\n",
    "evaluate_mitsl_words(globals()[this+\"_sample\"])\n",
    "print(\"--------------------------\")\n",
    "print(\"Generates such strings:\", globals()[this+\"_sample\"][:15])\n",
    "print(\"--------------------------\")\n",
    "print(\"Size of the grammar:\", len(globals()[this].grammar))\n",
    "print(\"--------------------------\")\n",
    "print(\"Grammars:\", globals()[this].grammar)'''\n",
    "\n",
    "\n",
    "acc16 = lambda w : evaluate_mitsl_words([w])#acceptor\n",
    "sigstar16 = list(star(set(''.join(assim_two)), 5))\n",
    "\n",
    "g16 = learn(assim_two + [\"\"], k=2,m=2)\n",
    "\n",
    "wrong16 = wrong(g16, acc16, sigstar16)\n",
    "\n",
    "save(16, assim_two, acc16, sigstar16, g16, wrong16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 190/364 1.0\n",
      "4 109/364 1.0\n",
      "7 493/1365 1.0\n",
      "8 213/3906 1.0\n",
      "9 390/66430 1.0\n",
      "12 143/1365 1.0\n",
      "13 864/3906 1.0\n",
      "14 7/63 1.0\n",
      "15 70/364 1.0\n",
      "16 30329/37449 1.0\n"
     ]
    }
   ],
   "source": [
    "acc = { #gather acceptor functions\n",
    "    1:evaluate_wfd_words,\n",
    "    4:lambda w : harmonic_evaluator(w, single_harmony_no_blockers),\n",
    "    7:lambda w : harmonic_evaluator(w, single_harmony_with_blockers),\n",
    "    8:lambda w : harmonic_evaluator(w, double_harmony),\n",
    "    9:lambda w : harmonic_evaluator(w, backness_and_rounding),\n",
    "    12:lambda w : harmonic_evaluator(w, double_harmony_no_blockers),\n",
    "    13:lambda w : harmonic_evaluator(w, double_harmony_with_blockers),\n",
    "    14:evaluate_utp_strings,\n",
    "    15:evaluate_first_last_words,\n",
    "    16:evaluate_mitsl_words\n",
    "}\n",
    "\n",
    "for i in [1, 4, 7, 8, 9, 12, 13, 14, 15, 16]: #for each experiment:\n",
    "    wr = pickle.load(open(f'{i}/wrong', 'rb'))          #load the list of strings for which the learned grammar gave the wrong result\n",
    "    sample = pickle.load(open(f'{i}/sample', 'rb'))     #load the sample it was trained on\n",
    "    sigma = set(''.join(sample))                        #gather every character in the target language\n",
    "    sigstar = list(star(sigma, 5))                      #generate every string up to length 5\n",
    "    print(i, f'{len(wr)}/{len(sigstar)}', acc[i](wr))   #print the language#, the fraction of strings up to length 5 the learned grammar failed on, and whether these failed words were accepted by the original acceptor\n",
    "\n",
    "#Here's the sample output from when I ran it:\n",
    "'''\n",
    "1 190/364 1.0\n",
    "4 109/364 1.0\n",
    "7 493/1365 1.0\n",
    "8 213/3906 1.0\n",
    "9 390/66430 1.0\n",
    "12 143/1365 1.0\n",
    "13 864/3906 1.0\n",
    "14 7/63 1.0\n",
    "15 70/364 1.0\n",
    "16 30329/37449 1.0\n",
    "'''\n",
    "\n",
    "#this rightmost column result tells us the grammar is incomplete, but consistent - it only failed on words that it should have accepted (but didn't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 []\n",
      "4 []\n",
      "7 []\n",
      "8 []\n",
      "9 []\n",
      "12 []\n",
      "13 []\n",
      "14 []\n",
      "15 []\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#NEXT: what if we take these \"failed\" strings, and train the learner on them? If these languages are learnable by this algorithm, then it should converge on the target grammar, \n",
    "\n",
    "for i in [1, 4, 7, 8, 9, 12, 13, 14, 15, 16]:               #For each experiment:\n",
    "                                                                        #load everything\n",
    "    wr = pickle.load(open(f'{i}/wrong', 'rb')) #failed strings\n",
    "    g = pickle.load(open(f'{i}/g', 'rb')) #grammar\n",
    "\n",
    "                                                                        #somehow sigstar didn't pickle correctly, so we need to rebuild it\n",
    "    sample = pickle.load(open(f'{i}/sample', 'rb'))\n",
    "    sigma = set(''.join(sample))\n",
    "    sigstar = list(star(sigma, 5))\n",
    "\n",
    "    g = learn(wr, g=g)                                                  #learn on the missing samples\n",
    "\n",
    "    accw = lambda w : acc[i]([w])                                       #converting the acceptor from the above cell (which takes a whole list as input) into an acceptor that wrong() can use, meaning it only takes one word at a time\n",
    "\n",
    "    wr = wrong(g, accw, sigstar)                                        #test it on sigstar and generate the new list of misses\n",
    "\n",
    "    print(i, wr)\n",
    "\n",
    "#Here is the sample output from when I ran it:\n",
    "'''\n",
    "1 []\n",
    "4 []\n",
    "7 []\n",
    "8 []\n",
    "9 []\n",
    "12 []\n",
    "13 []\n",
    "14 []\n",
    "15 []\n",
    "# Sadly, the Jupyter Kernel crashed after it spent over a day running experiment 16 \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c7f822a9fda7ffd10530ef71b8007e75e5e59461c46d04a19e3026085bccd1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
